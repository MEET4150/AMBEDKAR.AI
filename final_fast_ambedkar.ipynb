{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2J/b4T11Jif6TdLXJI79M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MEET4150/AMBEDKAR.AI/blob/master/final_fast_ambedkar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SdAGNjECwZKR"
      },
      "outputs": [],
      "source": [
        "from fastapi import FastAPI, File, UploadFile, Form\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "import os\n",
        "import PyPDF2\n",
        "import docx2txt\n",
        "from googletrans import Translator\n",
        "from google.generativeai import configure, GenerativeModel\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "# === Gemini Setup ===\n",
        "configure(api_key=\"AIzaSyBqk4WXN7k4UhjaBCzgSmuWn_bEor5aSyw\")  # Replace with your Gemini key\n",
        "model = GenerativeModel(\"gemini-2.0-flash\")\n",
        "\n",
        "# === System Instruction ===\n",
        "system_instruction = (\n",
        "    \"You are Ambedkar AI, a legal assistant exclusively trained on the Indian legal system.\\n\"\n",
        "    \"Your task is to interpret laws, judgments, and legal documents strictly according to Indian law.\\n\"\n",
        "    \"Never provide advice based on international laws. Always cite IPC, CrPC, or applicable Indian acts.\\n\"\n",
        ")\n",
        "\n",
        "# === User Token Plans ===\n",
        "USER_TOKENS = {\n",
        "    \"555\": 10000,\n",
        "    \"1111\": 200000,\n",
        "}\n",
        "token_usage = {}\n",
        "\n",
        "def check_token_limit(user_id, tokens_used):\n",
        "    allowed = USER_TOKENS.get(user_id)\n",
        "    if allowed is None:\n",
        "        return False, \"❌ Invalid plan ID.\"\n",
        "    used = token_usage.get(user_id, 0)\n",
        "    if used + tokens_used > allowed:\n",
        "        return False, f\"❌ Token limit exceeded: {used + tokens_used}/{allowed}\"\n",
        "    token_usage[user_id] = used + tokens_used\n",
        "    return True, f\"✅ Token usage: {token_usage[user_id]}/{allowed}\"\n",
        "\n",
        "def extract_text(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    try:\n",
        "        if ext == \".pdf\":\n",
        "            text = \"\"\n",
        "            with open(file_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                for page in reader.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text += page_text + \"\\n\"\n",
        "            return text\n",
        "        elif ext == \".docx\":\n",
        "            return docx2txt.process(file_path)\n",
        "        elif ext == \".txt\":\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return f.read()\n",
        "        else:\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] Failed to extract: {e}\"\n",
        "\n",
        "def translate_text(text, target_lang='en'):\n",
        "    try:\n",
        "        translator = Translator()\n",
        "        translated = translator.translate(text, dest=target_lang)\n",
        "        return translated.text\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] Translation failed: {e}\"\n",
        "\n",
        "def fetch_legal_news():\n",
        "    try:\n",
        "        url = \"https://www.barandbench.com/news\"\n",
        "        r = requests.get(url)\n",
        "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "        headlines = [h.get_text(strip=True) for h in soup.find_all(\"h3\")[:5]]\n",
        "        return \"\\n\".join(headlines) if headlines else \"No headlines found.\"\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching news: {e}\"\n",
        "\n",
        "def chat_gemini(prompt, user_id, doc_context=\"\"):\n",
        "    try:\n",
        "        context_note = f\"\\nRefer to the uploaded document:\\n{doc_context[:3000]}\\n\" if doc_context else \"\"\n",
        "        full_prompt = f\"{system_instruction}{context_note}User Query: {prompt}\"\n",
        "        response = model.generate_content(full_prompt)\n",
        "        tokens_used = len(full_prompt.split()) + len(response.text.split())\n",
        "        status, msg = check_token_limit(user_id, tokens_used)\n",
        "        if not status:\n",
        "            return {\"status\": \"error\", \"message\": msg}\n",
        "        return {\"status\": \"success\", \"message\": response.text, \"token_info\": msg}\n",
        "    except Exception as e:\n",
        "        return {\"status\": \"error\", \"message\": f\"Gemini Error: {e}\"}\n",
        "\n",
        "# === FastAPI App ===\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS middleware for frontend integration\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # Replace with your frontend domain in production\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class ChatRequest(BaseModel):\n",
        "    user_id: str\n",
        "    prompt: str\n",
        "    context: str = \"\"\n",
        "\n",
        "@app.post(\"/chat\")\n",
        "def chat_endpoint(req: ChatRequest):\n",
        "    return chat_gemini(req.prompt, req.user_id, req.context)\n",
        "\n",
        "@app.post(\"/upload\")\n",
        "async def upload_file(user_id: str = Form(...), file: UploadFile = File(...)):\n",
        "    file_location = f\"temp/{file.filename}\"\n",
        "    os.makedirs(\"temp\", exist_ok=True)\n",
        "    with open(file_location, \"wb\") as f:\n",
        "        f.write(await file.read())\n",
        "    extracted = extract_text(file_location)\n",
        "    os.remove(file_location)\n",
        "    if not extracted:\n",
        "        return {\"status\": \"error\", \"message\": \"❌ Unsupported file or empty.\"}\n",
        "    return {\"status\": \"success\", \"text\": extracted}\n",
        "\n",
        "@app.post(\"/translate\")\n",
        "async def translate_endpoint(text: str = Form(...), target_lang: str = Form(...)):\n",
        "    translated = translate_text(text, target_lang)\n",
        "    return {\"status\": \"success\", \"translated\": translated}\n",
        "\n",
        "@app.get(\"/news\")\n",
        "def news_endpoint():\n",
        "    headlines = fetch_legal_news()\n",
        "    return {\"status\": \"success\", \"news\": headlines}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tfXpRNNwwnye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22dda061",
        "outputId": "0b659174-cf98-4744-8f96-8086c77c6d6d"
      },
      "source": [
        "%pip install PyPDF2 docx2txt googletrans==4.0.0-rc1 beautifulsoup4 requests"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.9-py3-none-any.whl.metadata (529 bytes)\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2025.6.15)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2025.1.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docx2txt-0.9-py3-none-any.whl (4.0 kB)\n",
            "Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2025.1.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=2e28faeb0265181a9b9c22bf64304230ff194477036f82dc65be4693d24ad71b\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/17/6f/66a045ea3d168826074691b4b787b8f324d3f646d755443fda\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, docx2txt, chardet, PyPDF2, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: hyperframe\n",
            "    Found existing installation: hyperframe 6.1.0\n",
            "    Uninstalling hyperframe-6.1.0:\n",
            "      Successfully uninstalled hyperframe-6.1.0\n",
            "  Attempting uninstall: hpack\n",
            "    Found existing installation: hpack 4.1.0\n",
            "    Uninstalling hpack-4.1.0:\n",
            "      Successfully uninstalled hpack-4.1.0\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.16.0\n",
            "    Uninstalling h11-0.16.0:\n",
            "      Successfully uninstalled h11-0.16.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: h2\n",
            "    Found existing installation: h2 4.2.0\n",
            "    Uninstalling h2-4.2.0:\n",
            "      Successfully uninstalled h2-4.2.0\n",
            "  Attempting uninstall: httpcore\n",
            "    Found existing installation: httpcore 1.0.9\n",
            "    Uninstalling httpcore-1.0.9:\n",
            "      Successfully uninstalled httpcore-1.0.9\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.31.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "langsmith 0.4.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "google-genai 1.23.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.13.3 which is incompatible.\n",
            "gradio-client 1.10.1 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
            "openai 1.93.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.13.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyPDF2-3.0.1 chardet-3.0.4 docx2txt-0.9 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2025.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    }
  ]
}